{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "tokenize_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QueX6rO8wyOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "676c2f6e-3b09-413e-8319-2b314be1616b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = \"/content/drive/My Drive/Colab Notebooks/hypefyn/\" # <-- change this to your hypefyn folder"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFsSH83owhYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "7f94b7d9-c61b-4c1f-bcd1-be8421d9e686"
      },
      "source": [
        "import re, string, nltk, json\n",
        "import pandas as pd\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz6H-OfDwhY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(list):\n",
        "    tknzr = TweetTokenizer()\n",
        "    tokens_list = []\n",
        "    for text in list:\n",
        "        tokens_list.append(tknzr.tokenize(text))\n",
        "    return tokens_list\n",
        "\n",
        "def remove_noise(tokens_list, stop_words = (), keep_emph=False):\n",
        "\n",
        "    tokens = []\n",
        "    for tweet_tokens in tokens_list:\n",
        "        cleaned_tokens = []\n",
        "\n",
        "        for token, tag in pos_tag(tweet_tokens):\n",
        "            token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                        '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "            token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "            if tag.startswith(\"NN\"):\n",
        "                pos = 'n'\n",
        "            elif tag.startswith('VB'):\n",
        "                pos = 'v'\n",
        "            else:\n",
        "                pos = 'a'\n",
        "\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "            if keep_emph:\n",
        "                punct = re.sub(\"[!?]\",\"\", string.punctuation)\n",
        "            else:\n",
        "                punct = string.punctuation\n",
        "\n",
        "            if len(token) > 0 and token not in punct and token.lower() not in stop_words:\n",
        "                cleaned_tokens.append(token.lower())\n",
        "        tokens.append(cleaned_tokens)\n",
        "    return tokens\n",
        "\n",
        "def join_tokens(tokens_list):\n",
        "    joint = []\n",
        "    for tweet in tokens_list:\n",
        "        tw = \"\"\n",
        "        for word in tweet:\n",
        "            tw += \" \" + word\n",
        "        joint.append(tw)\n",
        "    return joint\n",
        "\n",
        "def dict_to_json(label, tokens_clean, output_path):\n",
        "    exp = {}\n",
        "    for ind, toks in enumerate(tokens_clean):\n",
        "        t = {}\n",
        "        t[\"label\"] = label[ind]\n",
        "        t[\"tokens\"] = toks\n",
        "        exp[ind] = t\n",
        "    with open(output_path,'w') as o:\n",
        "        json.dump(exp,o)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIHoKWgjwhY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "08cab03f-be40-4048-a63d-6e2661094da0"
      },
      "source": [
        "sentiment_path = drive_path + \"data/sentiment140/\"\n",
        "tweets_path = drive_path + \"data/\"\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "train_cols = [\"polarity\",\"id\",\"date\",\"query\",\"user\",\"text\"]\n",
        "train_data = pd.read_csv(sentiment_path+\"train.csv\", encoding='latin',header=None,names=train_cols)\n",
        "val_data = pd.read_csv(sentiment_path+\"test.csv\", encoding='latin',header=None,names=train_cols)\n",
        "\n",
        "tweets_data = pd.read_csv(tweets_path+\"all_tweets.csv\")\n",
        "print(\"Data Loaded\")\n",
        "\n",
        "train_tokens = tokenize(train_data.text.to_list())\n",
        "val_tokens = tokenize(val_data.text.to_list())\n",
        "tweets_tokens = tokenize(tweets_data.text.to_list())\n",
        "print(\"Data Tokenized\")\n",
        "\n",
        "train_tokens_clean = remove_noise(train_tokens, stop_words)\n",
        "print(\"Train data cleaned\")\n",
        "val_tokens_clean = remove_noise(val_tokens, stop_words)\n",
        "print(\"Val data cleaned\")\n",
        "tweets_tokens_clean = remove_noise(tweets_tokens, stop_words)\n",
        "print(\"Tweets data cleaned\")\n",
        "\n",
        "train_joint = join_tokens(train_tokens_clean)\n",
        "print(\"Train data joined\")\n",
        "val_joint = join_tokens(val_tokens_clean)\n",
        "print(\"Val data joined\")\n",
        "tweets_joint = join_tokens(tweets_tokens_clean)\n",
        "print(\"Tweets data joined\")\n",
        "\n",
        "\n",
        "dict_to_json(train_data.polarity.to_list(),train_joint, output_path=sentiment_path + \"train_clean_joint.json\")\n",
        "print(\"Train data exported\")\n",
        "dict_to_json(val_data.polarity.to_list(),val_joint, output_path=sentiment_path + \"val_clean_joint.json\")\n",
        "print(\"Val data exported\")\n",
        "dict_to_json(tweets_data.keyword.to_list(),tweets_joint, output_path=tweets_path + \"tweets_clean_joint.json\")\n",
        "print(\"Tweets data exported\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Loaded\n",
            "Data Tokenized\n",
            "Train data cleaned\n",
            "Val data cleaned\n",
            "Tweets data cleaned\n",
            "Train data joined\n",
            "Val data joined\n",
            "Tweets data joined\n",
            "Train data exported\n",
            "Val data joined\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-046cfc73a3e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mdict_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_joint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentiment_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"val_clean_joint.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Val data joined\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdict_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweets_joint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweets_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train_clean_joint.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tweets data joined\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'polarity'"
          ]
        }
      ]
    }
  ]
}