{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re, string, nltk, json\n","import pandas as pd\n","from nltk.tokenize import TweetTokenizer\n","from nltk.tag import pos_tag\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.corpus import stopwords\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tokenize(list):\n","    tknzr = TweetTokenizer()\n","    tokens_list = []\n","    for text in list:\n","        tokens_list.append(tknzr.tokenize(text))\n","    return tokens_list\n","\n","def remove_noise(tokens_list, stop_words = (), keep_emph=False):\n","\n","    tokens = []\n","    for tweet_tokens in tokens_list:\n","        cleaned_tokens = []\n","\n","        for token, tag in pos_tag(tweet_tokens):\n","            token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                        '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n","            token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n","\n","            if tag.startswith(\"NN\"):\n","                pos = 'n'\n","            elif tag.startswith('VB'):\n","                pos = 'v'\n","            else:\n","                pos = 'a'\n","\n","            lemmatizer = WordNetLemmatizer()\n","            token = lemmatizer.lemmatize(token, pos)\n","\n","            if keep_emph:\n","                punct = re.sub(\"[!?]\",\"\", string.punctuation)\n","            else:\n","                punct = string.punctuation\n","\n","            if len(token) > 0 and token not in punct and token.lower() not in stop_words:\n","                cleaned_tokens.append(token.lower())\n","        tokens.append(cleaned_tokens)\n","    return tokens\n","\n","def join_tokens(tokens_list):\n","    joint = []\n","    for tweet in tokens_list:\n","        tw = \"\"\n","        for word in tweet:\n","            tw += \" \" + word\n","        joint.append(tw)\n","    return joint\n","\n","def dict_to_json(label, tokens_clean, output_path):\n","    exp = {}\n","    for ind, toks in enumerate(tokens_clean):\n","        t = {}\n","        t[\"label\"] = label[ind]\n","        t[\"tokens\"] = toks\n","        exp[ind] = t\n","    with open(output_path,'w') as o:\n","        json.dump(exp,o)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","train_path = \"data/sentiment140/train.csv\"\n","val_path = \"data/sentiment140/test.csv\"\n","tweets_path = \"data/all_tweets.csv\"\n","\n","train_cols = [\"polarity\",\"id\",\"date\",\"query\",\"user\",\"text\"]\n","train_data = pd.read_csv(train_path, encoding='latin',header=None,names=train_cols)\n","val_data = pd.read_csv(val_path, encoding='latin',header=None,names=train_cols)\n","\n","# tweets_data = pd.read_csv(tweets_path)\n","\n","train_tokens = tokenize(train_data.text.to_list())\n","val_tokens = tokenize(val_data.text.to_list())\n","# tweets_tokens = tokenize(tweets_data.text.to_list())\n","\n","stop_words = stopwords.words('english')\n","\n","\n","\n","train_tokens_clean = remove_noise(train_tokens[:1000], stop_words)\n","# val_tokens_clean = remove_noise(val_tokens, stop_words)\n","# tweets_tokens_clean = remove_noise(tweets_tokens, stop_words)\n","\n","\n","\n","train_joint = join_tokens(train_tokens_clean)\n","\n","d = dictionalize(train_data.iloc[:1000].polarity.to_list(),train_tokens_clean)\n","\n","\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}